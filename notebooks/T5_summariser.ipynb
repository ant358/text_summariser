{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with a T5 for Text Summarisartion  \n",
    "\n",
    "Adapted from [Denis Rothman - Transformers for Natural Languge Processing](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition/blob/main/Chapter08/Summerizing_Text_with_T5.ipynb)  \n",
    "\n",
    "Trying the T5 large model first - need to find a way to score these models! (Bleu, Rouge, BERTSUM, etc))   \n",
    "And will it run locally on this machine?  (Yes, it does! see venv)  \n",
    "Might need to set up the GPUs (Not tested yet)  \n",
    "Note using the anaconda environment 'transformers'  (or text_sum_venv)  \n",
    "So far I have added: (will freeze a requirements.txt when everything is working):  \n",
    "`conda install -c conda-forge transformers`  \n",
    "`conda install -c pytorch pytorch`   \n",
    "`conda install -c conda-forge sentencepiece`  \n",
    "`conda install -c conda-forge tensorflow` not needed for this notebook but will need for other transformer experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import json \n",
    "import glob\n",
    "import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose to display the model config and architecture\n",
    "display_architecture=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 123kB/s]\n",
      "Downloading: 100%|██████████| 2.95G/2.95G [02:00<00:00, 24.6MB/s]\n",
      "Downloading: 100%|██████████| 792k/792k [00:00<00:00, 1.35MB/s]\n",
      "Downloading: 100%|██████████| 1.39M/1.39M [00:00<00:00, 2.16MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "# try cpu first its probably enough for this example 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model\n",
    "if display_architecture:\n",
    " print(model)\n",
    " # note all the repeated blocks are the same\n",
    " # can do model.encoder or .decoder or .forward to see the just those parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_architecture:\n",
    " print(model.config)\n",
    " # 16 heads and 24 layers - note the summarization prefix params!\n",
    " # note the beam search algo is being used  \n",
    " # there is a length penalty for longer sentences\n",
    " # vocab size is the size of the tokenizer vocab and can influence \n",
    " # the performance of the model, to large and it will be very sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../src/models/t5-large\\\\tokenizer_config.json',\n",
       " '../src/models/t5-large\\\\special_tokens_map.json',\n",
       " '../src/models/t5-large\\\\spiece.model',\n",
       " '../src/models/t5-large\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model \n",
    "model.save_pretrained(\"../src/models/t5-large\")\n",
    "# # save the tokenizer\n",
    "tokenizer.save_pretrained(\"../src/models/t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, ml):\n",
    "    \"\"\"\n",
    "    The function takes in a text and the max\n",
    "    length of the summary. It returns a summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text(str): the text to summarize\n",
    "    ml(int): the max length of the summary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns: the summary\n",
    "    \"\"\"\n",
    "    preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
    "    # add the prefix to the text\n",
    "    t5_prepared_Text = f\"summarize: {preprocess_text}\"\n",
    "    # eyeball the result of preprocessing\n",
    "    # print (\"Preprocessed and prepared text: \\n\", t5_prepared_Text)\n",
    "    # encode the text\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      # there are some very long sentences >512\n",
    "                                      truncation=True).to(device)\n",
    "    # submit the text to the model and adjust the parameters\n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                 num_beams=4,\n",
    "                                 no_repeat_ngram_size=2,\n",
    "                                 min_length=30,\n",
    "                                 max_length=ml,\n",
    "                                 early_stopping=True)\n",
    "    # decode the ids to text\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AGW_W540\\OneDrive\\Documents\\GitHub\\text_summariser\\txt_sum_venv\\lib\\site-packages\\transformers\\generation_utils.py:1818: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarized text: \n",
      " the united states declaration of independence was the first etext published by project gutenberg, early in 1971. the 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in\n"
     ]
    }
   ],
   "source": [
    "# small test\n",
    "text = \"\"\" The United States Declaration of Independence was the first Etext\n",
    "released by Project Gutenberg, early in 1971.  The title was stored\n",
    "in an emailed instruction set which required a tape or diskpack be\n",
    "hand mounted for retrieval.  The diskpack was the size of a large\n",
    "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
    "which this file took 1-2%.  Two tape backups were kept plus one on\n",
    "paper tape.  The 10,000 files we hope to have online by the end of\n",
    "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
    "\"\"\"\n",
    "print(\"Number of characters:\", len(text))\n",
    "summary = summarize(text, 50)\n",
    "print(\"\\nSummarized text: \\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the txt files in the directory\n",
    "txt_files = glob.glob(\"../text_data/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:31<13:41, 91.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bill of rights  which has  498  words \n",
      "Summarized text: \n",
      " the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated. no person shall be held to answer for a capital, or otherwise infamous crime, unless ona presentment or indictment of s. grand juries, except in cases arisingin the land or naval forces,or in the Militia, when in actual service in time of war or public danger ; and no one shallbe compelled in any criminal case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:50<11:15, 84.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Consitution of the United States of America  which has  4541  words \n",
      "Summarized text: \n",
      " all legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist ofa Senate and House of Representatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:27<10:29, 89.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Declaration of independence  which has  1349  words \n",
      "Summarized text: \n",
      " the thirteen united states ofamerica signed the declaration of independence on July 4, 1776. 'all men are created equal, that they are endowed by their Creator with certain unalienable Rights,' says john b. davis, jr. \"that to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed.' the present King of greatbritain is a history of repeatedinjuries and usurpations, all having in direct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:39<08:18, 83.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Give Me Liberty Or Give Me Death  which has  1223  words \n",
      "Summarized text: \n",
      " the questing before the House is one of awful moment to this country. for my part, i consider it as nothing less than a questionof freedom or slavery - and in proportion to the magnitude of the subjectought to be the freedom ofthe debate.\" 'i wish to know what there has been in the conductof the British ministry for the last ten years'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:03<06:56, 83.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " JFKs Inaugural Address  which has  1423  words \n",
      "Summarized text: \n",
      " the torch has been passed to a new generation of americans. let every nation know that we shall pay any price, bear any burden, meet any hardship, to assure the survival and the success of liberty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [08:10<05:10, 77.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lincolns first address  which has  3626  words \n",
      "Summarized text: \n",
      " oath taken by president \"before he enters on the execution of his office\" lincoln: \"there has never been any reasonable cause for such apprehension\" \"I have no purpose, directly or indirectly, to interfere with the institution of slavery\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [09:36<04:00, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lincolns Gettysburg Address  which has  298  words \n",
      "Summarized text: \n",
      " the world will little note, nor long remember,what we say here. but itcan never forget what they did here, far above ourpoor power to add ordetract. the brave men, living and dead,who struggled here have consecrated it,far beyond our poor powerto addordetect this ground. it is for us the living, rather, to be here dedicated to the great task remainingfor which they gave the last full measure of devotion. we here highly resolve that these dead shall not have died in va\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [11:06<02:47, 83.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lincolns Second Inaugural Address-AWynne  which has  703  words \n",
      "Summarized text: \n",
      " president obama delivered his second inaugural address on march 4, 1865. bob greene says the first address was devoted to averting civil war, but the second was to save the union without war - he says america is still in the midst of the greatcontest which still engrosses the energiesof the nation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [12:03<01:15, 75.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Lincolns Second Inaugural Address  which has  0  words \n",
      "Summarized text: \n",
      " april 1 marks the first day of the new year. the u.s. president's eu summit will be held in london on wednesday thursday, june 1 - if you're in the mood for relaxation, then the day is off – and the sun is out!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [12:57<00:00, 77.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mayflower compact  which has  303  words \n",
      "Summarized text: \n",
      " the mayflower compact was signed on the 11th of November, 1620. it was written by the Loyal Subjects of our dread sovereign Lord, King James, of Great Britaine, France, and Ireland,King, Defender of the Faith, &c.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loop through the files and summarize them\n",
    "for file in tqdm.tqdm(txt_files):\n",
    "    with open(file, 'r') as f:\n",
    "      text = f.read()\n",
    "      print(\"\\n\", file.split('\\\\')[-1].split(\".\")[0],\n",
    "            # get the number of words in the text\n",
    "            \" which has \", len(text.split()), \" words\",\n",
    "            \"\\nSummarized text: \\n\",\n",
    "            summarize(text, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f5556ecf21f3b0584f30edbf878c9d73d1a84532ffbc68f5d297a37f6a44b43"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
