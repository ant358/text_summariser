{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with a T5 for Text Summarisartion  \n",
    "\n",
    "Adapted from [Denis Rothman - Transformers for Natural Languge Processing](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition/blob/main/Chapter08/Summerizing_Text_with_T5.ipynb)  \n",
    "\n",
    "Trying the T5 large model first - need to find a way to score these models! (Bleu, Rouge, BERTSUM, etc))   \n",
    "And will it run locally on this machine?  (Yes, it does! see venv)  \n",
    "Might need to set up the GPUs (Not tested yet)  \n",
    "Note using the anaconda environment 'transformers'  (or text_sum_venv)  \n",
    "So far I have added: (will freeze a requirements.txt when everything is working):  \n",
    "`conda install -c conda-forge transformers`  \n",
    "`conda install -c pytorch pytorch`   \n",
    "`conda install -c conda-forge sentencepiece`  \n",
    "`conda install -c conda-forge tensorflow` not needed for this notebook but will need for other transformer experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AnthonyWynne\\code_workspace\\text_summarIser\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import json \n",
    "import glob\n",
    "import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose to display the model config and architecture\n",
    "display_architecture=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.21k/1.21k [00:00<00:00, 1.26MB/s]\n",
      "c:\\Users\\AnthonyWynne\\code_workspace\\text_summarIser\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AnthonyWynne\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 2.95G/2.95G [00:32<00:00, 91.8MB/s]\n",
      "Downloading: 100%|██████████| 792k/792k [00:00<00:00, 1.61MB/s]\n",
      "c:\\Users\\AnthonyWynne\\code_workspace\\text_summarIser\\venv\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "# try cpu first its probably enough for this example 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model\n",
    "if display_architecture:\n",
    " print(model)\n",
    " # note all the repeated blocks are the same\n",
    " # can do model.encoder or .decoder or .forward to see the just those parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_architecture:\n",
    " print(model.config)\n",
    " # 16 heads and 24 layers - note the summarization prefix params!\n",
    " # note the beam search algo is being used  \n",
    " # there is a length penalty for longer sentences\n",
    " # vocab size is the size of the tokenizer vocab and can influence \n",
    " # the performance of the model, to large and it will be very sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../src/models/t5-large\\\\tokenizer_config.json',\n",
       " '../src/models/t5-large\\\\special_tokens_map.json',\n",
       " '../src/models/t5-large\\\\spiece.model',\n",
       " '../src/models/t5-large\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model \n",
    "model.save_pretrained(\"../src/models/t5-large\")\n",
    "# # save the tokenizer\n",
    "tokenizer.save_pretrained(\"../src/models/t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, ml):\n",
    "    \"\"\"\n",
    "    The function takes in a text and the max\n",
    "    length of the summary. It returns a summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text(str): the text to summarize\n",
    "    ml(int): the max length of the summary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns: the summary\n",
    "    \"\"\"\n",
    "    preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
    "    # add the prefix to the text\n",
    "    t5_prepared_Text = f\"summarize: {preprocess_text}\"\n",
    "    # eyeball the result of preprocessing\n",
    "    # print (\"Preprocessed and prepared text: \\n\", t5_prepared_Text)\n",
    "    # encode the text\n",
    "    tokenized_text = tokenizer.encode(t5_prepared_Text,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      # there are some very long sentences >512\n",
    "                                      truncation=True).to(device)\n",
    "    # submit the text to the model and adjust the parameters\n",
    "    summary_ids = model.generate(tokenized_text,\n",
    "                                 num_beams=4,\n",
    "                                 no_repeat_ngram_size=2,\n",
    "                                 min_length=30,\n",
    "                                 max_length=ml,\n",
    "                                 early_stopping=True)\n",
    "    # decode the ids to text\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 534\n",
      "\n",
      "Summarized text: \n",
      " the united states declaration of independence was the first etext published by project gutenberg, early in 1971. the 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in\n"
     ]
    }
   ],
   "source": [
    "# small test\n",
    "text = \"\"\" The United States Declaration of Independence was the first Etext\n",
    "released by Project Gutenberg, early in 1971.  The title was stored\n",
    "in an emailed instruction set which required a tape or diskpack be\n",
    "hand mounted for retrieval.  The diskpack was the size of a large\n",
    "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
    "which this file took 1-2%.  Two tape backups were kept plus one on\n",
    "paper tape.  The 10,000 files we hope to have online by the end of\n",
    "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
    "\"\"\"\n",
    "print(\"Number of characters:\", len(text))\n",
    "summary = summarize(text, 50)\n",
    "print(\"\\nSummarized text: \\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the txt files in the directory\n",
    "txt_files = glob.glob(\"../text_data/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through the files and summarize them\n",
    "for file in tqdm.tqdm(txt_files):\n",
    "    with open(file, 'r') as f:\n",
    "      text = f.read()\n",
    "      print(\"\\n\", file.split('\\\\')[-1].split(\".\")[0],\n",
    "            # get the number of words in the text\n",
    "            \" which has \", len(text.split()), \" words\",\n",
    "            \"\\nSummarized text: \\n\",\n",
    "            summarize(text, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 534\n",
      "\n",
      "Summarized text: \n",
      " the u.s. declaration of independence was the first etext published by project gutenberg. the 10,000 files we hope to have\n"
     ]
    }
   ],
   "source": [
    "# small test\n",
    "text = \"\"\" The United States Declaration of Independence was the first Etext\n",
    "released by Project Gutenberg, early in 1971.  The title was stored\n",
    "in an emailed instruction set which required a tape or diskpack be\n",
    "hand mounted for retrieval.  The diskpack was the size of a large\n",
    "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
    "which this file took 1-2%.  Two tape backups were kept plus one on\n",
    "paper tape.  The 10,000 files we hope to have online by the end of\n",
    "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
    "\"\"\"\n",
    "print(\"Number of characters:\", len(text))\n",
    "summary = summarize(text, 30)\n",
    "print(\"\\nSummarized text: \\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f59b55dd257af911292aa61287bc9f9c7b6dccebb01953dfcb6b8ac7202107c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
